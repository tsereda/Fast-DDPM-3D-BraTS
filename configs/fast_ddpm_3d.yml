data:
    dataset: "BraTS3D"
    volume_size: [64, 64, 64]  # Reduced from 80 for memory safety
    channels: 1
    num_workers: 4

model:
    type: "unified_4to1"  # Corrected: 4 input modalities â†’ 1 target modality
    in_channels: 4        # Input: all 4 modalities
    out_ch: 1            # Output: single target modality
    ch: 64               # Reduced for 3D memory constraints
    ch_mult: [1, 2, 4]   # Reduced depth for 3D
    num_res_blocks: 2
    attn_resolutions: [16]  # Adjusted for 64x64x64 volumes
    dropout: 0.1
    var_type: fixedsmall
    ema_rate: 0.9999
    ema: True
    resamp_with_conv: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 1000

training:
    batch_size: 1        # Small batch size for 3D
    epochs: 1000
    learning_rate: 0.00002   # Even more conservative for late-training stability
    weight_decay: 0.01       # Add weight decay for regularization
    gradient_clip: 0.1       # Much tighter gradient clipping to prevent spikes
    save_every: 2000     # Save checkpoint every 2000 training steps (less frequent)
    validate_every: 1000 # Validate every 1000 training steps
    log_every_n_steps: 50
    # Gradient accumulation will be controlled via command line argument
    
    # Loss scaling settings for stability - very conservative to prevent spikes
    loss_scale_init: 128.0     # Even lower initial scale
    loss_scale_growth: 500     # More frequent scale adjustments
    loss_scale_backoff: 0.1    # Very aggressive scale reduction on overflow
    loss_scale_max: 1024.0     # Cap maximum scale to prevent runaway scaling
    
optim:
    weight_decay: 0.01       # Add weight decay for regularization
    optimizer: "Adam"
    lr: 0.00002              # Match training.learning_rate
    beta1: 0.9
    beta2: 0.999             # Default beta2 for better stability
    amsgrad: false
    eps: 0.00000001

# Additional stability settings
stability:
    warmup_steps: 200           # Shorter warmup since training is progressing
    memory_cleanup_interval: 25 # Very frequent cleanup to prevent memory issues
    nan_check_interval: 10      # Very frequent NaN checks
    max_loss_threshold: 50.0    # Much lower threshold to catch spikes early
    gradient_norm_threshold: 5.0 # Alert if gradient norm gets too high
    lr_schedule: "cosine_annealing_warm_restarts"  # Better LR schedule for stability