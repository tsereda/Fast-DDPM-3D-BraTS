data:
    dataset: "BraTS3D"
    volume_size: [64, 64, 64]  # Reduced from 80 for memory safety
    channels: 1
    num_workers: 4

model:
    type: "unified_4to1"  # Corrected: 4 input modalities â†’ 1 target modality
    in_channels: 4        # Input: all 4 modalities
    out_ch: 1            # Output: single target modality
    ch: 64               # Reduced for 3D memory constraints
    ch_mult: [1, 2, 4]   # Reduced depth for 3D
    num_res_blocks: 2
    attn_resolutions: [16]  # Adjusted for 64x64x64 volumes
    dropout: 0.1
    var_type: fixedsmall
    ema_rate: 0.9999
    ema: True
    resamp_with_conv: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 1000

training:
    batch_size: 1        # Small batch size for 3D
    epochs: 1000
    learning_rate: 0.00005   # Further reduced for stability
    weight_decay: 0.0
    gradient_clip: 0.5       # Reduced gradient clipping for better stability
    save_every: 2000     # Save checkpoint every 2000 training steps (less frequent)
    validate_every: 1000 # Validate every 1000 training steps
    log_every_n_steps: 50
    # Gradient accumulation will be controlled via command line argument
    
    # Loss scaling settings for stability - more conservative
    loss_scale_init: 512.0     # Much lower initial scale
    loss_scale_growth: 1000    # More frequent growth checks
    loss_scale_backoff: 0.25   # More aggressive scale reduction on overflow
    
optim:
    weight_decay: 0.0
    optimizer: "Adam"
    lr: 0.000025  # Further reduced to match training.learning_rate
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001

# Additional stability settings
stability:
    warmup_steps: 500           # Reduced warmup for faster convergence
    memory_cleanup_interval: 50 # More frequent cleanup for stability
    nan_check_interval: 25      # More frequent NaN checks
    max_loss_threshold: 500.0   # Alert threshold for large losses