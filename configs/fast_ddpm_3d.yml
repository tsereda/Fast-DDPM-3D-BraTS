# Fast-DDPM 3D Configuration for BraTS Modality Synthesis
# 4 input modalities → 1 target modality synthesis

data:
    dataset: "BraTS3D"
    volume_size: [100, 100, 100]  # Conservative for memory
    channels: 1               # Output channels (target modality)
    num_workers: 4

model:
    type: "unified_4to1"     # 4 input modalities → 1 target modality
    in_channels: 4           # Input: all 4 BraTS modalities
    out_ch: 1               # Output: single target modality
    ch: 64                  # Base channels (reduced for 3D memory)
    ch_mult: [1, 2, 4]      # Channel multipliers (3 levels for 3D)
    num_res_blocks: 2       # ResNet blocks per level
    attn_resolutions: [16]  # Attention at 16x16x16 resolution
    dropout: 0.1
    var_type: fixedsmall    # Fixed variance (not learned)
    ema_rate: 0.9999        # Exponential moving average
    ema: True
    resamp_with_conv: True  # Use conv for up/downsampling

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 1000

training:
    # Loss function: Using ultra_stable for problematic early training
    loss_type: "ultra_stable"       # Options: ultra_stable, stable, streamlined, enhanced, simple
    batch_size: 1                  # Small for 3D memory constraints (per GPU)
    gradient_accumulation_steps: 2   # Effective batch size = batch_size * world_size * gradient_accumulation_steps
    epochs: 1000
    learning_rate: 0.00005
    weight_decay: 0.0
    gradient_clip: 10.0             # Increased threshold for diffusion models
    
    # Learning rate scheduling
    warmup_steps: 1000              # Warmup for first 1000 steps
    warmup_start_lr: 0.000001       # Start with very low LR
    
    # Multi-GPU settings
    multi_gpu: true                 # Enable multi-GPU training
    distributed: false              # Use DataParallel (false) or DistributedDataParallel (true)
    sync_batchnorm: true           # Synchronize batch normalization across GPUs
    
    # Checkpointing
    save_every: 2000                # Save every 2000 steps
    validate_every: 1000            # Validate every 1000 steps
    log_every_n_steps: 50           # Log progress every 50 steps

optim:
    optimizer: "Adam"
    lr: 0.00001                     # Same as training.learning_rate
    beta1: 0.9
    weight_decay: 0.0
    amsgrad: false
    eps: 0.00000001

# Sampling configuration
sampling:
    timesteps: 10                   # Fast-DDPM: only 10 steps
    scheduler_type: "uniform"       # or "non-uniform"
    eta: 0.0                       # DDIM parameter
    batch_size: 2
    
    # Noise initialization
    init_type: "gaussian"          # Use Gaussian noise instead of zeros
    noise_std: 1.0                 # Standard deviation for Gaussian noise
    noise_mean: 0.0                # Mean for Gaussian noise

# Device and precision
device: "cuda"
mixed_precision: True              # Enable for memory efficiency