data:
    dataset: "BraTS3D"
    volume_size: [64, 64, 64]  # Reduced from 80 for memory safety
    channels: 1
    num_workers: 4

model:
    type: "unified_4to1"  # Corrected: 4 input modalities â†’ 1 target modality
    in_channels: 4        # Input: all 4 modalities
    out_ch: 1            # Output: single target modality
    ch: 64               # Reduced for 3D memory constraints
    ch_mult: [1, 2, 4]   # Reduced depth for 3D
    num_res_blocks: 2
    attn_resolutions: [16]  # Adjusted for 64x64x64 volumes
    dropout: 0.1
    var_type: fixedsmall
    ema_rate: 0.9999
    ema: True
    resamp_with_conv: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 1000

training:
    batch_size: 1        # Small batch size for 3D
    epochs: 1000
    learning_rate: 0.00001   # Further reduced due to high gradient norms
    weight_decay: 0.1        # Stronger regularization to control gradients
    gradient_clip: 0.05      # Much stricter clipping to prevent NaN gradients
    save_every: 2000     # Save checkpoint every 2000 training steps (less frequent)
    validate_every: 1000 # Validate every 1000 training steps
    log_every_n_steps: 50
    # Gradient accumulation will be controlled via command line argument
    
    # Loss scaling settings for stability - very conservative to prevent NaN
    loss_scale_init: 64.0      # Lower scale to prevent overflow
    loss_scale_growth: 2000    # Less frequent growth to maintain stability
    loss_scale_backoff: 0.05   # Very aggressive reduction on any issues
    loss_scale_max: 256.0      # Lower cap to prevent instability
    
optim:
    weight_decay: 0.1            # Stronger regularization to control gradients
    optimizer: "Adam"
    lr: 0.00001              # Match training.learning_rate - lower due to gradient issues
    beta1: 0.9
    beta2: 0.99              # Slightly lower beta2 for more stable gradient estimates
    amsgrad: false
    eps: 0.0000001           # Slightly larger epsilon for numerical stability

# Additional stability settings
stability:
    warmup_steps: 100           # Much shorter warmup to get to stable LR quickly
    memory_cleanup_interval: 10 # Very frequent cleanup to prevent accumulation
    nan_check_interval: 5       # Check for NaN every 5 steps
    max_loss_threshold: 10.0    # Lower threshold since losses are ~0.5
    gradient_norm_threshold: 1.0 # Much lower threshold - current norms are 5-16!
    lr_schedule: "cosine_annealing_warm_restarts"  # Better LR schedule for stability