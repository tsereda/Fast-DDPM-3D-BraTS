apiVersion: v1
kind: Pod
metadata:
  name: tsereda-brats-pod
  labels:
    app: fast-ddpm-brats
spec:
  nodeSelector:
    topology.kubernetes.io/region: us-west
  
  containers:
    - name: brats-processing
      image: gitlab-registry.nrp-nautilus.io/prp/jupyter-stack/prp
      
      env:
        - name: REPO_PATH
          value: /app/Fast-DDPM-3D-BraTS
        - name: PYTHONPATH
          value: /app/Fast-DDPM-3D-BraTS
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        # nnUNet paths
        - name: nnUNet_raw
          value: /app/nnunet/raw
        - name: nnUNet_preprocessed
          value: /app/nnunet/preprocessed
        - name: nnUNet_results
          value: /app/nnunet/results
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: PYTHONIOENCODING
          value: "UTF-8"
        # W&B configuration
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: wandb-credentials
              key: api-key  # Adjust this key name to match your secret
        - name: WANDB_PROJECT
          value: "fast-ddpm-3d-brats"
        - name: WANDB_ENTITY
          value: "timgsereda"  # Replace with your W&B username/team
        - name: WANDB_MODE
          value: "online"  # Change to "offline" if needed
      
      command: ["/bin/bash", "-c"]
      args:
        - |
          git clone -b dev3 https://github.com/tsereda/Fast-DDPM-3D-BraTS.git ${REPO_PATH}
          cd ${REPO_PATH}
          
          mkdir -p /app/nnunet/{raw,preprocessed,results}
          
          sudo apt-get update && sudo apt-get install -y p7zip-full wget git
          
          for dataset in "TrainingData" "ValidationData"; do
            zip_file="/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}.zip"
            if [ -f "$zip_file" ]; then
              echo "Extracting ${dataset}..."
              cd /data && 7z x "$zip_file" -y
              sudo chown -R jovyan:jovyan "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}"
              cd ${REPO_PATH}
              ln -sf "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}" .
            fi
          done
          
          pip install pyyaml torch tqdm numpy nibabel wandb matplotlib
          
          echo "Checking W&B configuration..."
          python -c "import wandb; print(f'W&B version: {wandb.__version__}')"
          python test_loss.py
          echo "Starting training with W&B logging..."
          # PYTHONUNBUFFERED=1 python scripts/train_3d.py \
          #   --data_root ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData \
          #   --use_wandb \
          #   --wandb_project ${WANDB_PROJECT} \
          #   --wandb_entity ${WANDB_ENTITY} \
          #   --doc pod-training-$(date +%Y%m%d-%H%M%S)
          
          tail -f /dev/null
      
      volumeMounts:
        - name: workspace
          mountPath: /app
        - name: data
          mountPath: /data
        - name: shm
          mountPath: /dev/shm
      
      resources:
        requests:
          memory: 28Gi
          cpu: "12"
          nvidia.com/gpu: "1"
        limits:
          memory: 32Gi
          cpu: "16"
          nvidia.com/gpu: "1"
  
  volumes:
    - name: workspace
      emptyDir:
        sizeLimit: 50Gi
    - name: data
      persistentVolumeClaim:
        claimName: brats2025-1
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
  
  restartPolicy: Never